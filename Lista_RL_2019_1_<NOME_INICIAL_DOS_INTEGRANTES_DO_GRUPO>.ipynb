{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lista_RL_2019-1_<NOME_INICIAL_DOS_INTEGRANTES_DO_GRUPO>.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girotodenis/semisupervisionada/blob/master/Lista_RL_2019_1_%3CNOME_INICIAL_DOS_INTEGRANTES_DO_GRUPO%3E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVTqvLaQRzch",
        "colab_type": "text"
      },
      "source": [
        "# Lista de Exercícios - Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_uFBUBgTl_z",
        "colab_type": "text"
      },
      "source": [
        "NOME COMPLETO DOS INTEGRANTES DO GRUPO:\n",
        "\n",
        "MATRÍCULAS DOS INTEGRANTES DO GRUPO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsxP3khToXi",
        "colab_type": "text"
      },
      "source": [
        "**AVISOS**:\n",
        "\n",
        "- **Mantenha a estrutura de enunciados e células de resposta da lista original na lista a ser entregue.**\n",
        "- **Todas as análises e justificativas solicitadas devem estar acompanhadas de códigos e plots realizados.**\n",
        "- **As listas deverão ser compartilhadas no e-mail nguerinjr@gmail.com .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDidH5uTsky",
        "colab_type": "text"
      },
      "source": [
        "# EXERCÍCIOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QeJRgXxfAE",
        "colab_type": "text"
      },
      "source": [
        "1) Considere o gridworld do exemplo apresentado no slide 48/70 do tópico 3, onde há 14 estados não-terminais e 2 estados terminais. Nesse cenário, a recompensa é -1 para todas as transições, inclusive para os estados terminais. Nos slides apresentados (que também pode ser visto no livro base da disciplina), é possível ver diferentes iterações da policy evaluation e qual seria a política greedy correspondente. Em apenas 3 iterações, já a política greedy já é a política ótima.\n",
        "\n",
        "Reimplemente o mesmo exemplo usando o algoritmo de policy iteration. Nesse caso, considere que cada etapa de policy evaluation terá apenas 1 iteração.\n",
        "\n",
        "**Responda**: o algoritmo consegue chegar na política ótima? Quantas iterações entre evaluation e improvement foram necessárias? O método convergiu mais rápido do que a policy evaluation do exemplo? Exiba a política greedy obtida em cada iteração da policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ZopZ2Yxesb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actions8 = [  \n",
        "                (-1, -1), (-1, 0), (-1, 1),\n",
        "                ( 0, -1),          ( 0, 1),\n",
        "                ( 1,  1), ( 1, 0), ( 1, 1)\n",
        "          ]\n",
        "pcardeais8 = [\n",
        "            'NO','N_','NE',\n",
        "            'O_',     'L_',\n",
        "            'SO','S_','SE']\n",
        "\n",
        "actions4 = [  \n",
        "                         (-1, 0),\n",
        "                (0, -1),          (0, 1),\n",
        "                         (1,  0)\n",
        "          ]\n",
        "\n",
        "pcardeais4 = [\n",
        "                 'N_',\n",
        "            'O_',     'L_',\n",
        "                 'S_']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmNsw2O5eSvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjTw8Jn0eTj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mundo:\n",
        "    \n",
        "  def __init__(self, y, x, i, f, pcardeais, actions, forca_vento_coluna):\n",
        "        \n",
        "      self.row = y \n",
        "      self.col = x\n",
        "      \n",
        "      self.pcardeais = pcardeais\n",
        "      self.actions = actions\n",
        "        \n",
        "      self.grade = [ (row, col) for row in range(self.row) for col in range(self.col)]\n",
        "      self.forca_vento_coluna = forca_vento_coluna\n",
        "      self.inicio = i\n",
        "      self.fim = f\n",
        "      \n",
        "      self.movimentos = []\n",
        "      \n",
        "  def is_inicio(self, posicao):\n",
        "      return posicao[0] == self.inicio[0] and posicao[1] == self.inicio[1]\n",
        "\n",
        "  def is_fim(self, posicao):\n",
        "      return posicao[0] == self.fim[0] and posicao[1] == self.fim[1]\n",
        "\n",
        "  def pcardeal_to_action(self, pcardeal):\n",
        "      indexes = [i for i,x in enumerate(self.pcardeais) if x == pcardeal]\n",
        "      return self.actions[indexes[0]]\n",
        "\n",
        "  def action_to_pcardeal(self, action):\n",
        "      indexes = [i for i,x in enumerate(self.actions) if x == action]\n",
        "      return self.pcardeais[indexes[0]]\n",
        "      \n",
        "  def mover(self, origem, movimento):\n",
        "      \n",
        "      destino = np.array(origem) + np.array(movimento)\n",
        "      \n",
        "      if -1 in list(destino) or self.col in list(destino):\n",
        "          self.movimentos.append( (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino))  )\n",
        "          return self.movimentos[-1]\n",
        "          #return (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) \n",
        "      \n",
        "      coluna_destino = destino[1]\n",
        "      forca_vento = self.forca_vento_coluna[coluna_destino]\n",
        "      \n",
        "      if forca_vento > 0:\n",
        "          for it in range(forca_vento):\n",
        "              destino = np.array(destino) + np.array(self.pcardeal_to_action('N_'))\n",
        "              if -1 in list(destino) or self.col in list(destino):\n",
        "                  self.movimentos.append( (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) )\n",
        "                  return self.movimentos[-1]\n",
        "                  #return (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino))\n",
        "      \n",
        "      self.movimentos.append( (tuple(destino), self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) )\n",
        "      return  self.movimentos[-1]\n",
        "    \n",
        "  def caminho_realizaso(self,posicao_inicio):\n",
        "      \n",
        "      tmp = [[ '__' for row in range(self.row)] for col in range(self.col)]\n",
        "      tmp = np.array(tmp)\n",
        "      tmp[posicao_inicio] = 'i_'\n",
        "      tmp[self.fim] = 'f_'\n",
        "      \n",
        "      \n",
        "      movs = []\n",
        "      if len(self.movimentos)>0 :\n",
        "          for mov in self.movimentos:\n",
        "              movs.append(mov[1])\n",
        "              if mov[0] is not None and tmp[mov[0]] !='f_':\n",
        "                  tmp[mov[0]] = mov[1]\n",
        "          print(tmp)\n",
        "      return movs\n",
        "      "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hThlMVg3nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a2bfa4b9-4a59-4c8e-ee3f-05cfc14aa13e"
      },
      "source": [
        "mundo = Mundo(4, 4, (0, 0), (3, 3), pcardeais4, actions4, [0,0,0,0])\n",
        "posicao, movimento, inicio, fim = mundo.mover((0, 0), mundo.pcardeal_to_action('L_'))\n",
        "print(inicio,fim)\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('O_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "print(inicio,fim)\n",
        "mundo.caminho_realizaso((0, 0))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "False True\n",
            "[['i_' 'L_' 'L_' '__']\n",
            " ['__' 'O_' 'S_' '__']\n",
            " ['__' 'S_' '__' '__']\n",
            " ['__' 'S_' 'L_' 'f_']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L_', 'L_', 'S_', 'O_', 'S_', 'S_', 'L_', 'L_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6py_P-Xx6Cq",
        "colab_type": "text"
      },
      "source": [
        "2) Escolha um gridworld de preferência dentre os constantes na library de gridworlds e treine um agente nesse gridworld usando o método de Monte Carlo. Exiba uma demonstração do agente andando pelo gridworld após o treinamento realizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dbBvsIx6gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063L1VbDd4P3",
        "colab_type": "text"
      },
      "source": [
        "3) Implemente agora o Sarsa no cenário **Windy Gridworld with King's Moves**. Esse cenário é o mesmo windy gridworld mas, dessa vez, há oito possíveis ações: as usuais e as ações nas diagonais.\n",
        "\n",
        "Compare os resultados no grid com o cenário do exercício 3. Responda com base nos experimentos e plots: ter a disposição mais ações ajuda a acelerar a convergência do algoritmo? O caminho escolhido pelo agente muda?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-d6GWgTG7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAGWFPwfGBZ",
        "colab_type": "text"
      },
      "source": [
        "4) Considere uma nova variação do windy gridworld com king's move. Assuma que o efeito do vento, quando há algum, é estocástico, algumas vezes variando 1 unidade da média especificada em cada coluna. Ou seja, 1/3 das vezes o movimento é dado exatamente por estes valores, como no exercício anterior, mas 1/3 das vezes você move-se 1 célula acima do esperado e no último 1/3 das vezes você move 1 célula abaixo do valor esperado para o vento naquela coluna. Exiba e analise a trajetória do agente nesse gridworld."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfZJx9L9THiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4eP-iYxauBa",
        "colab_type": "text"
      },
      "source": [
        "5) Implemente o cenário do \"The Cliff\", apresentado em sala de aula. Utilize tanto o Sars quanto o Q-learning. Faça comparações dos resultados com a política greedy do agente após 100, 1000 e 10000 episódios.\n",
        "\n",
        "OBS.: utilize uma estratégia epsilon-greedy para a dinâmica do agente. Especifique hiperparâmetros de forma a conseguir convergência do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVA177vrTIKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11qeuJwSlQ-",
        "colab_type": "text"
      },
      "source": [
        "<EM CONSTRUÇÃO>"
      ]
    }
  ]
}