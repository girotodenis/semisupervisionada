{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lista_RL_2019-1_<NOME_INICIAL_DOS_INTEGRANTES_DO_GRUPO>.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girotodenis/semisupervisionada/blob/master/Lista_RL_2019_1_%3CNOME_INICIAL_DOS_INTEGRANTES_DO_GRUPO%3E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVTqvLaQRzch",
        "colab_type": "text"
      },
      "source": [
        "# Lista de Exercícios - Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_uFBUBgTl_z",
        "colab_type": "text"
      },
      "source": [
        "NOME COMPLETO DOS INTEGRANTES DO GRUPO:\n",
        "\n",
        "MATRÍCULAS DOS INTEGRANTES DO GRUPO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsxP3khToXi",
        "colab_type": "text"
      },
      "source": [
        "**AVISOS**:\n",
        "\n",
        "- **Mantenha a estrutura de enunciados e células de resposta da lista original na lista a ser entregue.**\n",
        "- **Todas as análises e justificativas solicitadas devem estar acompanhadas de códigos e plots realizados.**\n",
        "- **As listas deverão ser compartilhadas no e-mail nguerinjr@gmail.com .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDidH5uTsky",
        "colab_type": "text"
      },
      "source": [
        "# EXERCÍCIOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ZopZ2Yxesb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actions8 = [  \n",
        "                (-1, -1), (-1, 0), (-1, 1),\n",
        "                ( 0, -1),          ( 0, 1),\n",
        "                ( 1,  1), ( 1, 0), ( 1, 1)\n",
        "          ]\n",
        "pcardeais8 = [\n",
        "            'NO','N_','NE',\n",
        "            'O_',     'L_',\n",
        "            'SO','S_','SE']\n",
        "\n",
        "actions4 = [  \n",
        "                         (-1, 0),\n",
        "                (0, -1),          (0, 1),\n",
        "                         (1,  0)\n",
        "          ]\n",
        "\n",
        "pcardeais4 = [\n",
        "                 'N_',\n",
        "            'O_',     'L_',\n",
        "                 'S_']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmNsw2O5eSvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjTw8Jn0eTj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mundo:\n",
        "    \n",
        "  def __init__(self, y, x, i, f, pcardeais, actions, forca_vento_coluna):\n",
        "        \n",
        "      self.row = y \n",
        "      self.col = x\n",
        "      \n",
        "      self.pcardeais = pcardeais\n",
        "      self.actions = actions\n",
        "        \n",
        "      self.grade = [ (row, col) for row in range(self.row) for col in range(self.col)]\n",
        "      self.forca_vento_coluna = forca_vento_coluna\n",
        "      self.inicio = i\n",
        "      self.fim = f\n",
        "      \n",
        "      self.movimentos = []\n",
        "      \n",
        "  def is_inicio(self, posicao):\n",
        "      return posicao[0] == self.inicio[0] and posicao[1] == self.inicio[1]\n",
        "\n",
        "  def is_fim(self, posicao):\n",
        "      return posicao[0] == self.fim[0] and posicao[1] == self.fim[1]\n",
        "\n",
        "  def pcardeal_to_action(self, pcardeal):\n",
        "      indexes = [i for i,x in enumerate(self.pcardeais) if x == pcardeal]\n",
        "      return self.actions[indexes[0]]\n",
        "\n",
        "  def action_to_pcardeal(self, action):\n",
        "      indexes = [i for i,x in enumerate(self.actions) if x == action]\n",
        "      return self.pcardeais[indexes[0]]\n",
        "      \n",
        "  def mover(self, origem, movimento):\n",
        "      \n",
        "      destino = np.array(origem) + np.array(movimento)\n",
        "      \n",
        "      if -1 in list(destino) or self.col in list(destino):\n",
        "          self.movimentos.append( (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino))  )\n",
        "          return self.movimentos[-1]\n",
        "          #return (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) \n",
        "      \n",
        "      coluna_destino = destino[1]\n",
        "      forca_vento = self.forca_vento_coluna[coluna_destino]\n",
        "      \n",
        "      if forca_vento > 0:\n",
        "          for it in range(forca_vento):\n",
        "              destino = np.array(destino) + np.array(self.pcardeal_to_action('N_'))\n",
        "              if -1 in list(destino) or self.col in list(destino):\n",
        "                  self.movimentos.append( (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) )\n",
        "                  return self.movimentos[-1]\n",
        "                  #return (None, self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino))\n",
        "      \n",
        "      self.movimentos.append( (tuple(destino), self.action_to_pcardeal(movimento), self.is_inicio(destino), self.is_fim(destino)) )\n",
        "      return  self.movimentos[-1]\n",
        "  \n",
        "  def limpar_caminho_realizaso(self):\n",
        "        self.movimentos = []  \n",
        "  \n",
        "  def caminho_realizaso(self,posicao_inicio):\n",
        "      \n",
        "      tmp = [[ '__' for row in range(self.row)] for col in range(self.col)]\n",
        "      tmp = np.array(tmp)\n",
        "      tmp[posicao_inicio] = 'i_'\n",
        "      tmp[self.fim] = 'f_'\n",
        "      \n",
        "      \n",
        "      movs = []\n",
        "      if len(self.movimentos)>0 :\n",
        "          for mov in self.movimentos:\n",
        "              movs.append(mov[1])\n",
        "              if mov[0] is not None and tmp[mov[0]] !='f_':\n",
        "                  tmp[mov[0]] = mov[1]\n",
        "          print(tmp)\n",
        "      return movs\n",
        "      "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hThlMVg3nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9728e4ad-2f75-4985-8595-b4fe9f875c17"
      },
      "source": [
        "mundo = Mundo(4, 4, (0, 0), (3, 3), pcardeais4, actions4, [0,0,0,0])\n",
        "posicao, movimento, inicio, fim = mundo.mover((0, 0), mundo.pcardeal_to_action('L_'))\n",
        "print(inicio,fim)\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('O_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('S_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "posicao, movimento, inicio, fim = mundo.mover(posicao, mundo.pcardeal_to_action('L_'))\n",
        "print(inicio,fim)\n",
        "print(mundo.caminho_realizaso((0, 0)))\n",
        "mundo.limpar_caminho_realizaso()\n",
        "mundo.caminho_realizaso((0, 0))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "False True\n",
            "[['i_' 'L_' 'L_' '__']\n",
            " ['__' 'O_' 'S_' '__']\n",
            " ['__' 'S_' '__' '__']\n",
            " ['__' 'S_' 'L_' 'f_']]\n",
            "['L_', 'L_', 'S_', 'O_', 'S_', 'S_', 'L_', 'L_']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQJ8REXKptPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agente:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.gamma = 1.0\n",
        "\n",
        "        #Rt = -1 -> em todas transições\n",
        "        self.reward_value = -1\n",
        "        self.alpha = 0.5 \n",
        "        self.epsilon = 0.1 \n",
        "        self.iterations = 10000\n",
        "        self.Q = None\n",
        "    \n",
        "    def takeAction(self, posicao, action, mundo):\n",
        "        \n",
        "        if mundo.is_fim(tuple(posicao)):\n",
        "            return 0, None\n",
        "        nova_posicao, movimento, inicio, fim = mundo.mover(posicao, action)\n",
        "        \n",
        "        #saiu do mundo - Ações que levam o agente para fora do grid deixam o estado inalterado;\n",
        "        if nova_posicao is None or inicio:\n",
        "            return -10, posicao\n",
        "        \n",
        "        #Rt = -1 -> em todas transições\n",
        "        return self.reward_value, tuple(nova_posicao)\n",
        "        \n",
        "    \n",
        "    def learn(self, mundo, Algoritimo):\n",
        "        \n",
        "        deltas = {(row, col):list() for row in range(mundo.row) for col in range(mundo.col)}\n",
        "        \n",
        "        tmp = [[Algoritimo(mundo.actions, row, col) for row in range(mundo.row)] for col in range(mundo.col)]\n",
        "        self.Q = np.array(tmp)\n",
        "        \n",
        "        for it in range(self.iterations):\n",
        "\n",
        "            posicao = tuple(random.choice(mundo.grade))\n",
        "            \n",
        "            posicao_i = posicao\n",
        "            \n",
        "            mundo.limpar_caminho_realizaso()\n",
        "            \n",
        "            sub = 0\n",
        "            while True:\n",
        "                sub += 1\n",
        "                \n",
        "                if sub>1000:\n",
        "                    break\n",
        "            \n",
        "                current_cell = self.Q[posicao]\n",
        "                \n",
        "                #print(current_cell)\n",
        "                \n",
        "                action = current_cell.explorar(self.epsilon, mundo.actions)\n",
        "                reward, nova_posicao = self.takeAction(posicao, action, mundo)\n",
        "                \n",
        "                if nova_posicao is None:\n",
        "                    break\n",
        "                \n",
        "                next_cell =  self.Q[nova_posicao] \n",
        "                old_cell_value = current_cell.value\n",
        "                current_cell.learn(reward, next_cell, self.alpha, self.gamma )\n",
        "                \n",
        "                deltas[posicao].append(float(np.abs(old_cell_value - current_cell.value)))\n",
        "                \n",
        "                posicao = nova_posicao\n",
        "                \n",
        "        print('fim.')\n",
        "        all_series = [list(x)[:50] for x in deltas.values()]\n",
        "        return all_series\n",
        "    \n",
        "    def melhor_caminho(self,posicao_inicio, mundo):\n",
        "\n",
        "        #posicao = mundo.inicio\n",
        "        posicao = posicao_inicio\n",
        "        \n",
        "        mundo.limpar_caminho_realizaso()\n",
        "        \n",
        "        for it in range(20):\n",
        "            \n",
        "            current_cell = self.Q[posicao]\n",
        "            \n",
        "            action = current_cell.explorar(-1, mundo.actions)\n",
        "            nova_posicao, movimento, inicio, fim = mundo.mover(posicao, action)\n",
        "            \n",
        "            if nova_posicao is None or fim:\n",
        "                break\n",
        "            \n",
        "            next_cell =  self.Q[nova_posicao] \n",
        "            posicao = nova_posicao\n",
        "            \n",
        "        return mundo.caminho_realizaso(posicao_inicio)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QeJRgXxfAE",
        "colab_type": "text"
      },
      "source": [
        "1) Considere o gridworld do exemplo apresentado no slide 48/70 do tópico 3, onde há 14 estados não-terminais e 2 estados terminais. Nesse cenário, a recompensa é -1 para todas as transições, inclusive para os estados terminais. Nos slides apresentados (que também pode ser visto no livro base da disciplina), é possível ver diferentes iterações da policy evaluation e qual seria a política greedy correspondente. Em apenas 3 iterações, já a política greedy já é a política ótima.\n",
        "\n",
        "Reimplemente o mesmo exemplo usando o algoritmo de policy iteration. Nesse caso, considere que cada etapa de policy evaluation terá apenas 1 iteração.\n",
        "\n",
        "**Responda**: o algoritmo consegue chegar na política ótima? Quantas iterações entre evaluation e improvement foram necessárias? O método convergiu mais rápido do que a policy evaluation do exemplo? Exiba a política greedy obtida em cada iteração da policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RX6dIJnswgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyImprovement:\n",
        "    def __init__(self, actions, row, col):\n",
        "        self.cell = (row, col)\n",
        "        self.value = 0\n",
        "        \n",
        "    def explorar(self, epsilon, actions ):\n",
        "        action = self.explore(actions)\n",
        "        return action  \n",
        "    \n",
        "    def explore(self, actions):\n",
        "        action = random.choice(actions);\n",
        "        self.indexActions = actions.index(action)\n",
        "        return action\n",
        "    \n",
        "    def learn(self, reward_value, cell_final_state, alpha, gamma ):\n",
        "        # Iteração de Valor (Value Iteration)\n",
        "        #exemplo ???????\n",
        "        self.value += alpha * (reward_value + gamma * cell_final_state.value - self.value)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R4eUTcDsZds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "88a8dc0e-fae4-4b3e-baf4-59d8be0b5d32"
      },
      "source": [
        "mundo = Mundo(4, 4, (0, 0), (3, 3), pcardeais4, actions4, [0,0,0,0])\n",
        "agente = Agente()\n",
        "melhor_caminho = agente.learn(mundo, PolicyImprovement)\n",
        "print(agente.melhor_caminho(mundo.inicio, mundo))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fim.\n",
            "[['i_' '__' '__' '__']\n",
            " ['__' '__' '__' '__']\n",
            " ['__' '__' '__' '__']\n",
            " ['__' '__' '__' 'f_']]\n",
            "['N_']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6py_P-Xx6Cq",
        "colab_type": "text"
      },
      "source": [
        "2) Escolha um gridworld de preferência dentre os constantes na library de gridworlds e treine um agente nesse gridworld usando o método de Monte Carlo. Exiba uma demonstração do agente andando pelo gridworld após o treinamento realizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dbBvsIx6gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063L1VbDd4P3",
        "colab_type": "text"
      },
      "source": [
        "3) Implemente agora o Sarsa no cenário **Windy Gridworld with King's Moves**. Esse cenário é o mesmo windy gridworld mas, dessa vez, há oito possíveis ações: as usuais e as ações nas diagonais.\n",
        "\n",
        "Compare os resultados no grid com o cenário do exercício 3. Responda com base nos experimentos e plots: ter a disposição mais ações ajuda a acelerar a convergência do algoritmo? O caminho escolhido pelo agente muda?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-d6GWgTG7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAGWFPwfGBZ",
        "colab_type": "text"
      },
      "source": [
        "4) Considere uma nova variação do windy gridworld com king's move. Assuma que o efeito do vento, quando há algum, é estocástico, algumas vezes variando 1 unidade da média especificada em cada coluna. Ou seja, 1/3 das vezes o movimento é dado exatamente por estes valores, como no exercício anterior, mas 1/3 das vezes você move-se 1 célula acima do esperado e no último 1/3 das vezes você move 1 célula abaixo do valor esperado para o vento naquela coluna. Exiba e analise a trajetória do agente nesse gridworld."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfZJx9L9THiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4eP-iYxauBa",
        "colab_type": "text"
      },
      "source": [
        "5) Implemente o cenário do \"The Cliff\", apresentado em sala de aula. Utilize tanto o Sars quanto o Q-learning. Faça comparações dos resultados com a política greedy do agente após 100, 1000 e 10000 episódios.\n",
        "\n",
        "OBS.: utilize uma estratégia epsilon-greedy para a dinâmica do agente. Especifique hiperparâmetros de forma a conseguir convergência do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVA177vrTIKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11qeuJwSlQ-",
        "colab_type": "text"
      },
      "source": [
        "<EM CONSTRUÇÃO>"
      ]
    }
  ]
}