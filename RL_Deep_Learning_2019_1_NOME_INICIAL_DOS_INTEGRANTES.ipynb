{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Deep_Learning_2019-1_NOME_INICIAL_DOS_INTEGRANTES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girotodenis/semisupervisionada/blob/master/RL_Deep_Learning_2019_1_NOME_INICIAL_DOS_INTEGRANTES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVTqvLaQRzch",
        "colab_type": "text"
      },
      "source": [
        "# Lista de Exercícios - Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_uFBUBgTl_z",
        "colab_type": "text"
      },
      "source": [
        "NOME COMPLETO DOS INTEGRANTES DO GRUPO: Denis Silva Giroto\n",
        "\n",
        "MATRÍCULAS DOS INTEGRANTES DO GRUPO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsxP3khToXi",
        "colab_type": "text"
      },
      "source": [
        "**AVISOS**:\n",
        "\n",
        "- **Mantenha a estrutura de enunciados e células de resposta da lista original na lista a ser entregue.**\n",
        "- **Todas as análises e justificativas solicitadas devem estar acompanhadas de códigos e plots realizados.**\n",
        "- **As listas deverão ser compartilhadas no e-mail nguerinjr@gmail.com .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDidH5uTsky",
        "colab_type": "text"
      },
      "source": [
        "# EXERCÍCIOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgtUdIxTz7I",
        "colab_type": "text"
      },
      "source": [
        "## EXERCÍCIO 1\n",
        "\n",
        "Refere-se aos exemplos de código apresentados em sala de aula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lHNEUf9T7V2",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h68sx73JX8GP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Com o passar as iterações / experiências do agente, as correções aplicadas à função de valor tendem a cair. Nos exemplos, essa variação era denotada por um delta. Dessa maneira, pode ser mais interessante eficiente impor uma restrição com um delta, que denota a convergência do algoritmo, ao invés de obedecer cegamente ao número de iterações específicados.\n",
        "\n",
        "Faça uma versão do algoritmo de programação dinâmica e do de Monte Carlo que recebe um número desejado de iterações mas que tem como critério de convergência um delta, testado com relação a limiar de parada. Rode experimentos variando o limiar de delta e exiba os resultados da política greedy obtida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-JMlM5gTC6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6QOVUHnYAXu",
        "colab_type": "text"
      },
      "source": [
        "### 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DH-PDLnVx8v",
        "colab_type": "text"
      },
      "source": [
        "Implemente no exemplo de TD(0) uma estratégia epsilon-greedy, ao invés de uma escolha aleatória de ações. Para decidir qual a ação greedy em um determinado momento, é necessário considerar a maior função de valor nas redondezas de um estado.\n",
        "\n",
        "Faça experimentos variando o epsilon. Compare os resultados da política greedy após 10, 100, 500 e 1000 episódios, em cada um dos cenários. \n",
        "\n",
        "Compare esses resultados com os resultados de uma política greedy após 10, 100, 500 e 1000 episódios considerando o código original de exemplo.\n",
        "\n",
        "Responda com base nos experimentos e plots apresentados: qual o impacto do fator de exploração do agente ao usar o TD(0)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMQLGugPTE9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FYuOLq3ZHUe",
        "colab_type": "text"
      },
      "source": [
        "## EXERCÍCIO 2\n",
        "\n",
        "Diz respeito aos exemplos teóricos formulados em sala de aula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGQJNykZLQX",
        "colab_type": "text"
      },
      "source": [
        "### 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlMb8m5UZMQR",
        "colab_type": "text"
      },
      "source": [
        "Implemente o Sarsa para o exemplo do grid com o vento apresentado em sala de aula (ver exemplo e explicações detalhadas no livro referência). \n",
        "\n",
        "Lembrese: a diferença do Sars é que ao invés de usar a função valor-estado (função v), utiliza-se a função de estado-ação (função Q).\n",
        "\n",
        "Implemente uma estratégia epsilon-greedy e escolha os hiperparâmetros de forma a haver convergência do agente. Faça o experimento com 50, 100 e 1000 e 10000 episódios. Observe a política greedy do agente após esse número de episódios.\n",
        "\n",
        "Responda com base nos experimentos e plots: o agente consegue chegar de maneira interessante ao objetivo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01zATcObTFwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IQ9qSovaeXv",
        "colab_type": "text"
      },
      "source": [
        "### 2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkH_7z7af2W",
        "colab_type": "text"
      },
      "source": [
        "Utilize o algoritmo de Q-learning para o cenário 2.1. Há diferença na política greedy após 50, 100, 1000 e 10000 episódios com relação aos resultados obtidos em 2.1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9axgTMdTGWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtB3RrIJd19t",
        "colab_type": "text"
      },
      "source": [
        "### 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063L1VbDd4P3",
        "colab_type": "text"
      },
      "source": [
        "Considere o melhor resultado de 2.1 como baseline de comparação.\n",
        "\n",
        "Implemente agora o Sars no cenário **Windy Gridworld with King's Moves**, conforme especificado no Exercício 6.9 do livro base.\n",
        "\n",
        "Esse cenário é o mesmo windy gridworld mas, dessa vez, há oito possíveis ações: as usuais e as ações nas diagonais.\n",
        "\n",
        "Compare os resultados no grid nas mesmas circunstâncias. Responda com base nos experimentos e plots: ter a disposição mais ações ajuda a acelerar a convergência do algoritmo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-d6GWgTG7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaBMgTxDfFjH",
        "colab_type": "text"
      },
      "source": [
        "### 2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAGWFPwfGBZ",
        "colab_type": "text"
      },
      "source": [
        "Considere o algoritmo de Q-learning. Dessa vez modele um cenário de **Stochastic Windy Gridworld** conforme apresentado no exerício 6.10 do livro base.\n",
        "\n",
        "Nesse cenário, o vento, ao invés de ter um impacto definido deterministicamente, tem um comportamento estocástico: pode mover o agente 1 célula para cima, 2 células para cima ou 1 célula para baixo. Cada uma dessas situações tem uma probabilidade 1/3 de acontecer.\n",
        "\n",
        "Por exemplo, se você está á direita do estado objetivo e escolhe a ação \"esquerda\", em 1/3 das vezes você terminará uma célula acima do objetivo, em 1/3 das vezes você terminará duas células acima do objetivo e em 1/3 das vezes você terminará 1 célula abaixo do objetivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfZJx9L9THiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZSCIgPUasub",
        "colab_type": "text"
      },
      "source": [
        "### 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4eP-iYxauBa",
        "colab_type": "text"
      },
      "source": [
        "Implemente o cenário do \"The Cliff\", apresentado em sala de aula. Utilize tanto o Sars quanto o Q-learning. Faça comparações dos resultados com a política greedy do agente após 100, 1000 e 10000 episódios.\n",
        "\n",
        "OBS.: utilize uma estratégia epsilon-greedy para a dinâmica do agente. Especifique hiperparâmetros de forma a conseguir convergência do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVA177vrTIKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAxt9khQSjTJ",
        "colab_type": "text"
      },
      "source": [
        "# Exercício 3 <EM CONSTRUÇÃO>\n",
        "\n",
        "Uso de modelos de aprendizado por reforço profundo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11qeuJwSlQ-",
        "colab_type": "text"
      },
      "source": [
        "# Exercício 4 <EM CONSTRUÇÃO>\n",
        "\n",
        "Experimentos com autoencoders."
      ]
    }
  ]
}