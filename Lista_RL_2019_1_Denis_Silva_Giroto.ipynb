{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lista_RL_2019-1_<NOME_INICIAL_DOS_INTEGRANTES_DO_GRUPO>.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girotodenis/semisupervisionada/blob/master/Lista_RL_2019_1_Denis_Silva_Giroto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVTqvLaQRzch",
        "colab_type": "text"
      },
      "source": [
        "# Lista de Exercícios - Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_uFBUBgTl_z",
        "colab_type": "text"
      },
      "source": [
        "NOME COMPLETO DOS INTEGRANTES DO GRUPO:\n",
        "\n",
        "MATRÍCULAS DOS INTEGRANTES DO GRUPO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsxP3khToXi",
        "colab_type": "text"
      },
      "source": [
        "**AVISOS**:\n",
        "\n",
        "- **Mantenha a estrutura de enunciados e células de resposta da lista original na lista a ser entregue.**\n",
        "- **Todas as análises e justificativas solicitadas devem estar acompanhadas de códigos e plots realizados.**\n",
        "- **As listas deverão ser compartilhadas no e-mail nguerinjr@gmail.com .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDidH5uTsky",
        "colab_type": "text"
      },
      "source": [
        "# EXERCÍCIOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QeJRgXxfAE",
        "colab_type": "text"
      },
      "source": [
        "1) Considere o gridworld do exemplo apresentado no slide 48/70 do tópico 3, onde há 14 estados não-terminais e 2 estados terminais. Nesse cenário, a recompensa é -1 para todas as transições, inclusive para os estados terminais. Nos slides apresentados (que também pode ser visto no livro base da disciplina), é possível ver diferentes iterações da policy evaluation e qual seria a política greedy correspondente. Em apenas 3 iterações, já a política greedy já é a política ótima.\n",
        "\n",
        "Reimplemente o mesmo exemplo usando o algoritmo de policy iteration. Nesse caso, considere que cada etapa de policy evaluation terá apenas 1 iteração.\n",
        "\n",
        "**Responda**: o algoritmo consegue chegar na política ótima? Quantas iterações entre evaluation e improvement foram necessárias? O método convergiu mais rápido do que a policy evaluation do exemplo? Exiba a política greedy obtida em cada iteração da policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ZopZ2Yxesb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6py_P-Xx6Cq",
        "colab_type": "text"
      },
      "source": [
        "2) Escolha um gridworld de preferência dentre os constantes na library de gridworlds e treine um agente nesse gridworld usando o método de Monte Carlo. Exiba uma demonstração do agente andando pelo gridworld após o treinamento realizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dbBvsIx6gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063L1VbDd4P3",
        "colab_type": "text"
      },
      "source": [
        "3) Implemente agora o Sarsa no cenário **Windy Gridworld with King's Moves**. Esse cenário é o mesmo windy gridworld mas, dessa vez, há oito possíveis ações: as usuais e as ações nas diagonais.\n",
        "\n",
        "Compare os resultados no grid com o cenário do exercício 3. Responda com base nos experimentos e plots: ter a disposição mais ações ajuda a acelerar a convergência do algoritmo? O caminho escolhido pelo agente muda?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-d6GWgTG7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAGWFPwfGBZ",
        "colab_type": "text"
      },
      "source": [
        "4) Considere uma nova variação do windy gridworld com king's move. Assuma que o efeito do vento, quando há algum, é estocástico, algumas vezes variando 1 unidade da média especificada em cada coluna. Ou seja, 1/3 das vezes o movimento é dado exatamente por estes valores, como no exercício anterior, mas 1/3 das vezes você move-se 1 célula acima do esperado e no último 1/3 das vezes você move 1 célula abaixo do valor esperado para o vento naquela coluna. Exiba e analise a trajetória do agente nesse gridworld."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfZJx9L9THiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4eP-iYxauBa",
        "colab_type": "text"
      },
      "source": [
        "5) Implemente o cenário do \"The Cliff\", apresentado em sala de aula. Utilize tanto o Sars quanto o Q-learning. Faça comparações dos resultados com a política greedy do agente após 100, 1000 e 10000 episódios.\n",
        "\n",
        "OBS.: utilize uma estratégia epsilon-greedy para a dinâmica do agente. Especifique hiperparâmetros de forma a conseguir convergência do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVA177vrTIKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Células de código aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11qeuJwSlQ-",
        "colab_type": "text"
      },
      "source": [
        "<EM CONSTRUÇÃO>"
      ]
    }
  ]
}